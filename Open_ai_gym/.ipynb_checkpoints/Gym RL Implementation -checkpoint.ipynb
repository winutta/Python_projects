{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create a RL Algo which can 'solve' the CartPole Game Environment\n",
    "\n",
    "So Far, I have implemented exploration decay (epsilon decay), increasing batch size, Prioritized experience replay using recency as rank (I would like to change this to TD error ranking). This is in essence improving the weak spots\n",
    "\n",
    "Other things to try: Double DQN, Dueling DQN, SARSA, PPO (proximal policy optimization), Policy Gradient, Natural Policy Gradient, KL-Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "from collections import deque\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "# This method seems to overtrain so that only one action (1) is predicted\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12, input_shape = (1,observation_space,), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(action_space,activation = 'linear'))\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.045\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "# This method seems to overtrain so that only one action (1) is predicted\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(12, input_shape = (1,observation_space,), activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model1.add(Dense(32, activation = 'relu'))\n",
    "model1.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model1.add(Dense(12, activation = 'relu'))\n",
    "model1.add(Dense(action_space,activation = 'linear'))\n",
    "model1.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Count: 1, length of game: 13\n",
      "Game Count: 2, length of game: 15\n",
      "Game Count: 3, length of game: 15\n",
      "Game Count: 4, length of game: 19\n",
      "Game Count: 5, length of game: 33\n",
      "Game Count: 6, length of game: 15\n",
      "Game Count: 7, length of game: 11\n",
      "Game Count: 8, length of game: 23\n",
      "Game Count: 9, length of game: 13\n",
      "Game Count: 10, length of game: 11\n",
      "Game Count: 11, length of game: 12\n",
      "Game Count: 12, length of game: 12\n",
      "Game Count: 13, length of game: 25\n",
      "Game Count: 14, length of game: 27\n",
      "Game Count: 15, length of game: 13\n",
      "Game Count: 16, length of game: 36\n",
      "Game Count: 17, length of game: 14\n",
      "Game Count: 18, length of game: 14\n",
      "Game Count: 19, length of game: 13\n",
      "Game Count: 20, length of game: 14\n",
      "Game Count: 21, length of game: 17\n",
      "Game Count: 22, length of game: 34\n",
      "Game Count: 23, length of game: 22\n",
      "Game Count: 24, length of game: 13\n",
      "Game Count: 25, length of game: 10\n",
      "Game Count: 26, length of game: 37\n",
      "Game Count: 27, length of game: 28\n",
      "Game Count: 28, length of game: 13\n",
      "Game Count: 29, length of game: 17\n",
      "Game Count: 30, length of game: 25\n",
      "Game Count: 31, length of game: 25\n",
      "Game Count: 32, length of game: 31\n",
      "Game Count: 33, length of game: 36\n",
      "Game Count: 34, length of game: 24\n",
      "Game Count: 35, length of game: 11\n",
      "Game Count: 36, length of game: 46\n",
      "Game Count: 37, length of game: 15\n",
      "Game Count: 38, length of game: 13\n",
      "Game Count: 39, length of game: 19\n",
      "Game Count: 40, length of game: 28\n",
      "Game Count: 41, length of game: 41\n",
      "Game Count: 42, length of game: 11\n",
      "Game Count: 43, length of game: 17\n",
      "Game Count: 44, length of game: 21\n",
      "Game Count: 45, length of game: 11\n",
      "Game Count: 46, length of game: 34\n",
      "Game Count: 47, length of game: 19\n",
      "Game Count: 48, length of game: 33\n",
      "Game Count: 49, length of game: 24\n",
      "Game Count: 50, length of game: 30\n",
      "Game Count: 51, length of game: 21\n",
      "Game Count: 52, length of game: 27\n",
      "Game Count: 53, length of game: 10\n",
      "Game Count: 54, length of game: 24\n",
      "Game Count: 55, length of game: 28\n",
      "Game Count: 56, length of game: 27\n",
      "Game Count: 57, length of game: 30\n",
      "Game Count: 58, length of game: 25\n",
      "Game Count: 59, length of game: 11\n",
      "Game Count: 60, length of game: 16\n",
      "Game Count: 61, length of game: 11\n",
      "Game Count: 62, length of game: 70\n",
      "Game Count: 63, length of game: 36\n",
      "Game Count: 64, length of game: 12\n",
      "Game Count: 65, length of game: 50\n",
      "Game Count: 66, length of game: 21\n",
      "Game Count: 67, length of game: 15\n",
      "Game Count: 68, length of game: 27\n",
      "Game Count: 69, length of game: 63\n",
      "Game Count: 70, length of game: 13\n",
      "Game Count: 71, length of game: 13\n",
      "Game Count: 72, length of game: 18\n",
      "Game Count: 73, length of game: 40\n",
      "Game Count: 74, length of game: 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c31aab1ffa77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_mem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mq_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_count1 = 0\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]\n",
    "#print(observation_space)\n",
    "\n",
    "state = env.reset()\n",
    "memory1 = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "s = 0\n",
    "save = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0][0])\n",
    "    #r = (100)/(game_count+1)\n",
    "    r = -(1/(6000**2))*(game_count1**2)+1\n",
    "    #r = (1-((game_count1+1)/(game_count1+100)))\n",
    "    #r = 0.99**(game_count1)\n",
    "    r = np.clip(r,0,0.99)\n",
    "    # r could also be (game_count+1)/(game_count+100)\n",
    "    if r <= 0:\n",
    "        r = np.random.uniform(0,0.001)\n",
    "    if np.random.uniform(0,1) < r :\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count1 += 1\n",
    "        #print(_-save)\n",
    "       \n",
    "        print('Game Count: ' + str(game_count1) + ', length of game: ' + str(_-save))\n",
    "        save = _\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1,observation_space])\n",
    "    memory1.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory1) >= 2:\n",
    "        rank = np.arange(len(memory1)-1,0,-1)\n",
    "        #print(rank)\n",
    "        rank = 1/rank\n",
    "        rank = rank**0.6\n",
    "        #print(rank)\n",
    "        total = np.sum(rank)\n",
    "        numerator = rank\n",
    "        dist = numerator/total\n",
    "        \n",
    "        \n",
    "        cum_dist = [0]*len(dist)\n",
    "        cum_dist[0] = dist[0]\n",
    "        for i in range(1,len(dist)):\n",
    "            cum_dist[i] = cum_dist[i-1] + dist[i]\n",
    "        #print(cum_dist)\n",
    "        unif = np.random.uniform(0,1,size = int(np.ceil(len(memory1)**0.5)))\n",
    "        choices = []\n",
    "        for u in range(len(unif)):\n",
    "            choi = 0 \n",
    "            for j in range(len(cum_dist)):\n",
    "                if unif[u] > cum_dist[j]:\n",
    "                    choi += 1\n",
    "            choices.append(choi)\n",
    "        #print(choices)    \n",
    "        mem_choices = []\n",
    "        for q in choices:\n",
    "            mem_choices.append(memory1[q])\n",
    "        #plt.bar(x = range(0,len(dist)), height = dist)\n",
    "        #plt.show()\n",
    "        #temp_mem = random.sample(memory1,20)\n",
    "        temp_mem = mem_choices\n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model1.predict(new_state)[0][0]))\n",
    "            q_vals = model1.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            states.append(state)\n",
    "            model1.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\Anaconda3\\envs\\py36\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Game Count: 1527, length of game: 0\n",
      "Saved model to disk\n",
      "Game Count: 1528, length of game: 11\n",
      "Saved model to disk\n",
      "Game Count: 1529, length of game: 17\n",
      "Saved model to disk\n",
      "Game Count: 1530, length of game: 12\n",
      "Saved model to disk\n",
      "Game Count: 1531, length of game: 17\n",
      "Saved model to disk\n",
      "Game Count: 1532, length of game: 18\n",
      "Saved model to disk\n",
      "Game Count: 1533, length of game: 19\n",
      "Saved model to disk\n",
      "Game Count: 1534, length of game: 12\n",
      "Saved model to disk\n",
      "Game Count: 1535, length of game: 44\n",
      "Saved model to disk\n",
      "Game Count: 1536, length of game: 11\n",
      "Saved model to disk\n",
      "Game Count: 1537, length of game: 42\n",
      "Saved model to disk\n",
      "Game Count: 1538, length of game: 18\n",
      "Saved model to disk\n",
      "Game Count: 1539, length of game: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-92658614c6ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_mem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mq_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Continuous train!\n",
    "s = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "save = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0])\n",
    "     #r = (100)/(game_count+1)\n",
    "    r = -(1/(6000**2))*(game_count1**2)+1\n",
    "    #r = (1-((game_count1+1)/(game_count1+100)))\n",
    "    #r = 0.99**(game_count1)\n",
    "    r = np.clip(r,0,0.98)\n",
    "    # r could also be (game_count+1)/(game_count+100)\n",
    "    if np.random.uniform(0,1) < r :\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count1 += 1\n",
    "        model_json = model1.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model1.save_weights(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        print('Game Count: ' + str(game_count1) + ', length of game: ' + str(_-save))\n",
    "        save = _\n",
    "        if game_count1 == 10000:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    memory1.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory1) >= 2:\n",
    "        rank = np.arange(len(memory1)-1,0,-1)\n",
    "        #print(rank)\n",
    "        rank = 1/rank\n",
    "        #print(rank)\n",
    "        total = np.sum(rank**0.6)\n",
    "        numerator = rank**0.6\n",
    "        dist = numerator/total\n",
    "        \n",
    "        \n",
    "        cum_dist = [0]*len(dist)\n",
    "        cum_dist[0] = dist[0]\n",
    "        for i in range(1,len(dist)):\n",
    "            cum_dist[i] = cum_dist[i-1] + dist[i]\n",
    "        #print(cum_dist)\n",
    "        unif = np.random.uniform(0,1,size = int(np.ceil(len(memory1)**0.5)))\n",
    "        choices = []\n",
    "        for u in range(len(unif)):\n",
    "            choi = 0 \n",
    "            for j in range(len(cum_dist)):\n",
    "                if unif[u] > cum_dist[j]:\n",
    "                    choi += 1\n",
    "            choices.append(choi)\n",
    "        #print(choices)    \n",
    "        mem_choices = []\n",
    "        for q in choices:\n",
    "            mem_choices.append(memory1[q])\n",
    "        #plt.bar(x = range(0,len(dist)), height = dist)\n",
    "        #plt.show()\n",
    "        #temp_mem = random.sample(memory1,20)\n",
    "        temp_mem = mem_choices\n",
    "        #q_val_list = []\n",
    "        #states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model1.predict(new_state)[0][0]))\n",
    "            q_vals = model1.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            #q_val_list.append(q_vals)\n",
    "            #states.append(state)\n",
    "            model1.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 46.153656409866386 22.0 50.8\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "#game_state_save = game_count1\n",
    "#game_count1 = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "save = 0\n",
    "lens = []\n",
    "for _ in range(300):\n",
    "    state = np.reshape(state, [1,1, observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0])\n",
    "    #print(model1.predict(state))\n",
    "    #print(action)\n",
    "    \n",
    "    env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        #print(_-save)\n",
    "        lens.append(_-save)\n",
    "        save = _\n",
    "        \n",
    "        #env.close()\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    #memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "env.close()\n",
    "plt.hist(lens)\n",
    "plt.show()\n",
    "print(np.max(lens),np.std(lens),np.median(lens),np.mean(lens))\n",
    "#game_count1 = game_state_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_count = 0\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]\n",
    "#print(observation_space)\n",
    "\n",
    "state = env.reset()\n",
    "memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "s = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model.predict(state)[0][0])\n",
    "    if np.random.uniform(0,1) > 0.9:\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count += 1\n",
    "        print(game_count)\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1,observation_space])\n",
    "    memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory) >= 20:\n",
    "        temp_mem = random.sample(memory,20)\n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model.predict(new_state)[0][0]))\n",
    "            q_vals = model.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            states.append(state)\n",
    "            model.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous train!\n",
    "s = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    if np.random.uniform(0,1) > (game_count+1)/(game_count+100):\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count += 1\n",
    "        print(game_count)\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory) >= 20:\n",
    "        temp_mem = random.sample(memory,20)\n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model.predict(new_state)[0][0]))\n",
    "            q_vals = model.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            states.append(state)\n",
    "            model.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "\n",
    "for _ in range(1000):\n",
    "    state = np.reshape(state, [1,1, observation_space])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    print(model.predict(state))\n",
    "    print(action)\n",
    "    env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        #print(_)\n",
    "        #env.close()\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    #memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
