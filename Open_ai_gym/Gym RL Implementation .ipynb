{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create a RL Algo which can 'solve' the CartPole Game Environment\n",
    "\n",
    "So Far, I have implemented exploration decay (epsilon decay), increasing batch size, Prioritized experience replay using recency as rank (I would like to change this to TD error ranking). This is in essence improving the weak spots\n",
    "\n",
    "Other things to try: Double DQN, Dueling DQN, SARSA, PPO (proximal policy optimization), Policy Gradient, Natural Policy Gradient, KL-Divergence, Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Questions: Batch size Modulation/Constant, Learning Rate Modulation, Epsilon Rate Modulation/Constant, online vs batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "from collections import deque\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "from keras.models import load_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "# This method seems to overtrain so that only one action (1) is predicted\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12, input_shape = (1,observation_space,), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(action_space,activation = 'linear'))\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.045\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "# This method seems to overtrain so that only one action (1) is predicted\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(12, input_shape = (1,observation_space,), activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model1.add(Dense(32, activation = 'relu'))\n",
    "model1.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model1.add(Dense(12, activation = 'relu'))\n",
    "model1.add(Dense(action_space,activation = 'linear'))\n",
    "model1.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "model3 = model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.045\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "# This method seems to overtrain so that only one action (1) is predicted\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(12, input_shape = (1,observation_space,), activation=\"relu\"))\n",
    "model3.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model3.add(Dense(32, activation = 'relu'))\n",
    "model3.add(BatchNormalization())\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model3.add(Dense(12, activation = 'relu'))\n",
    "model3.add(Dense(action_space,activation = 'linear'))\n",
    "model3.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Count: 1, length of game: 24\n",
      "Game Count: 2, length of game: 10\n",
      "Game Count: 3, length of game: 26\n",
      "Game Count: 4, length of game: 20\n",
      "Game Count: 5, length of game: 38\n",
      "Game Count: 6, length of game: 11\n",
      "Game Count: 7, length of game: 20\n",
      "Game Count: 8, length of game: 19\n",
      "Game Count: 9, length of game: 74\n",
      "Game Count: 10, length of game: 27\n",
      "Game Count: 11, length of game: 74\n",
      "Game Count: 12, length of game: 19\n",
      "Game Count: 13, length of game: 28\n",
      "Game Count: 14, length of game: 17\n",
      "Game Count: 15, length of game: 36\n",
      "Game Count: 16, length of game: 13\n",
      "Game Count: 17, length of game: 16\n",
      "Game Count: 18, length of game: 19\n",
      "Game Count: 19, length of game: 21\n",
      "Game Count: 20, length of game: 12\n",
      "Game Count: 21, length of game: 30\n",
      "Game Count: 22, length of game: 20\n",
      "Game Count: 23, length of game: 22\n",
      "Game Count: 24, length of game: 24\n",
      "Game Count: 25, length of game: 13\n",
      "Game Count: 26, length of game: 27\n",
      "Game Count: 27, length of game: 16\n",
      "Game Count: 28, length of game: 22\n",
      "Game Count: 29, length of game: 23\n",
      "Game Count: 30, length of game: 22\n",
      "Game Count: 31, length of game: 39\n",
      "Game Count: 32, length of game: 23\n",
      "Game Count: 33, length of game: 11\n",
      "Game Count: 34, length of game: 16\n",
      "Game Count: 35, length of game: 54\n",
      "Game Count: 36, length of game: 14\n",
      "Game Count: 37, length of game: 17\n",
      "Game Count: 38, length of game: 11\n",
      "Game Count: 39, length of game: 23\n",
      "Game Count: 40, length of game: 16\n",
      "Game Count: 41, length of game: 32\n",
      "Game Count: 42, length of game: 19\n",
      "Game Count: 43, length of game: 33\n",
      "Game Count: 44, length of game: 10\n",
      "Game Count: 45, length of game: 36\n",
      "Game Count: 46, length of game: 40\n",
      "Game Count: 47, length of game: 10\n",
      "Game Count: 48, length of game: 16\n",
      "Game Count: 49, length of game: 37\n",
      "Game Count: 50, length of game: 28\n",
      "Game Count: 51, length of game: 15\n",
      "Game Count: 52, length of game: 33\n",
      "Game Count: 53, length of game: 18\n",
      "Game Count: 54, length of game: 48\n",
      "Game Count: 55, length of game: 11\n",
      "Game Count: 56, length of game: 25\n",
      "Game Count: 57, length of game: 22\n",
      "Game Count: 58, length of game: 20\n",
      "Game Count: 59, length of game: 53\n",
      "Game Count: 60, length of game: 14\n",
      "Game Count: 61, length of game: 36\n",
      "Game Count: 62, length of game: 25\n",
      "Game Count: 63, length of game: 29\n",
      "Game Count: 64, length of game: 32\n",
      "Game Count: 65, length of game: 10\n",
      "Game Count: 66, length of game: 13\n",
      "Game Count: 67, length of game: 36\n",
      "Game Count: 68, length of game: 20\n",
      "Game Count: 69, length of game: 25\n",
      "Game Count: 70, length of game: 28\n",
      "Game Count: 71, length of game: 23\n",
      "Game Count: 72, length of game: 13\n",
      "Game Count: 73, length of game: 20\n",
      "Game Count: 74, length of game: 11\n",
      "Game Count: 75, length of game: 19\n",
      "Game Count: 76, length of game: 43\n",
      "Game Count: 77, length of game: 23\n",
      "Game Count: 78, length of game: 11\n",
      "Game Count: 79, length of game: 24\n",
      "Game Count: 80, length of game: 14\n",
      "Game Count: 81, length of game: 14\n",
      "Game Count: 82, length of game: 19\n",
      "Game Count: 83, length of game: 18\n",
      "Game Count: 84, length of game: 20\n",
      "Game Count: 85, length of game: 32\n",
      "Game Count: 86, length of game: 47\n",
      "Game Count: 87, length of game: 38\n",
      "Game Count: 88, length of game: 25\n",
      "Game Count: 89, length of game: 34\n",
      "Game Count: 90, length of game: 13\n",
      "Game Count: 91, length of game: 54\n",
      "Game Count: 92, length of game: 17\n",
      "Game Count: 93, length of game: 10\n",
      "Game Count: 94, length of game: 12\n",
      "Game Count: 95, length of game: 17\n",
      "Game Count: 96, length of game: 14\n",
      "Game Count: 97, length of game: 26\n",
      "Game Count: 98, length of game: 21\n",
      "Game Count: 99, length of game: 25\n",
      "Game Count: 100, length of game: 13\n",
      "Game Count: 101, length of game: 22\n",
      "Game Count: 102, length of game: 38\n",
      "Game Count: 103, length of game: 21\n",
      "Game Count: 104, length of game: 43\n",
      "Game Count: 105, length of game: 24\n",
      "Game Count: 106, length of game: 30\n",
      "Game Count: 107, length of game: 21\n",
      "Game Count: 108, length of game: 15\n",
      "Game Count: 109, length of game: 9\n",
      "Game Count: 110, length of game: 14\n",
      "Game Count: 111, length of game: 15\n",
      "Game Count: 112, length of game: 33\n",
      "Game Count: 113, length of game: 21\n",
      "Game Count: 114, length of game: 13\n",
      "Game Count: 115, length of game: 28\n",
      "Game Count: 116, length of game: 39\n",
      "Game Count: 117, length of game: 86\n",
      "Game Count: 118, length of game: 9\n",
      "Game Count: 119, length of game: 22\n",
      "Game Count: 120, length of game: 18\n",
      "Game Count: 121, length of game: 12\n",
      "Game Count: 122, length of game: 21\n",
      "Game Count: 123, length of game: 10\n",
      "Game Count: 124, length of game: 13\n",
      "Game Count: 125, length of game: 23\n",
      "Game Count: 126, length of game: 29\n",
      "Game Count: 127, length of game: 21\n",
      "Game Count: 128, length of game: 54\n",
      "Game Count: 129, length of game: 17\n",
      "Game Count: 130, length of game: 23\n",
      "Game Count: 131, length of game: 24\n",
      "Game Count: 132, length of game: 25\n",
      "Game Count: 133, length of game: 12\n"
     ]
    }
   ],
   "source": [
    "game_count1 = 0\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]\n",
    "#print(observation_space)\n",
    "\n",
    "state = env.reset()\n",
    "memory1 = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "s = 0\n",
    "save = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0][0])\n",
    "    #r = (100)/(game_count+1)\n",
    "    r = -(1/(6000**2))*(game_count1**2)+1\n",
    "    #r = (1-((game_count1+1)/(game_count1+100)))\n",
    "    #r = 0.99**(game_count1)\n",
    "    r = np.clip(r,0.02,1)\n",
    "    # r could also be (game_count+1)/(game_count+100)\n",
    "\n",
    "    if np.random.uniform(0,1) < r :\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    new_state = np.reshape(new_state, [1,1,observation_space])\n",
    "    memory1.append([new_state,state,reward,action])\n",
    "    state = new_state\n",
    "    #print(memory1)\n",
    "    memory_len = len(memory1)\n",
    "    if ((memory_len >= 20)):\n",
    "        \n",
    "        rank = np.arange(memory_len,0,-1)\n",
    "        #print(rank)\n",
    "        rank = 1/(rank**0.6)\n",
    "        #rank = rank**0.6\n",
    "        #print(rank)\n",
    "        dist = rank/np.sum(rank)\n",
    "        \n",
    "        #print(memory_len)\n",
    "        cum_dist = np.cumsum(dist)\n",
    "        '''\n",
    "        cum_dist = [0]*memory_len\n",
    "        #print(dist)\n",
    "        #print(cum_dist)\n",
    "        cum_dist[0] = dist[0]\n",
    "        for i in range(1,len(cum_dist)):\n",
    "            #print(cum_dist[i-1])\n",
    "            #print(dist[i])\n",
    "            cum_dist[i] = cum_dist[i-1] + dist[i]\n",
    "        #print(cum_dist)\n",
    "        '''\n",
    "        p = int(np.ceil(memory_len**(2/np.pi)))\n",
    "        p = np.clip(p,1,20)\n",
    "        p = int(p)\n",
    "        \n",
    "        unif = np.random.uniform(0,1,size = p)\n",
    "        choices = []\n",
    "        for u in range(p):\n",
    "            choi = 0 \n",
    "            for j in range(p):\n",
    "                if unif[u] > cum_dist[j]:\n",
    "                    choi += 1\n",
    "                else:\n",
    "                    break\n",
    "            choices.append(choi)\n",
    "        #print(choices)    \n",
    "        temp_mem = [memory1[i] for i in choices]\n",
    "        #for q in choices:\n",
    "        #    temp_mem.append(memory1[q])\n",
    "        #plt.bar(x = range(0,len(dist)), height = dist)\n",
    "        #plt.show()\n",
    "        #temp_mem = random.sample(memory1,20)\n",
    "        q_val_list = deque(maxlen = p)\n",
    "        #states = deque(maxlen = int(p))\n",
    "        #_val_list = []\n",
    "        states = [i[1] for i in temp_mem]\n",
    "        #print(states)\n",
    "        #rint(temp_mem)\n",
    "        \n",
    "        #model1.save('my_model3.h5')\n",
    "        \n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            \n",
    "            if done == True:\n",
    "                update = reward\n",
    "            else:\n",
    "                update = reward + 0.95*(np.amax(model3.predict(new_state)[0][0]))\n",
    "            q_vals = model3.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            #states.append(state)\n",
    "            #model1.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        states = np.reshape(states, (p,1,4))\n",
    "        q_val_list = np.reshape(q_val_list, (p,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        model1.fit(states,q_val_list,epochs = len(temp_mem),verbose = 0)\n",
    "        \n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count1 += 1\n",
    "        #print(_-save)\n",
    "        K.clear_session()\n",
    "        model1 = load_model('my_model3.h5')\n",
    "        model3 = model1\n",
    "\n",
    "        print('Game Count: ' + str(game_count1) + ', length of game: ' + str(_-save))\n",
    "        save = _\n",
    "        state = env.reset()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Continuous train!\n",
    "s = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "save = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0])\n",
    "     #r = (100)/(game_count+1)\n",
    "    r = -(1/(6000**2))*(game_count1**2)+1\n",
    "    #r = (1-((game_count1+1)/(game_count1+100)))\n",
    "    #r = 0.99**(game_count1)\n",
    "    r = np.clip(r,0,0.98)\n",
    "    # r could also be (game_count+1)/(game_count+100)\n",
    "    if np.random.uniform(0,1) < r :\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "   \n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    memory1.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    \n",
    "    if ((len(memory1) >= 2) & (_%4 == 0)):\n",
    "        rank = np.arange(len(memory1)-1,0,-1)\n",
    "        #print(rank)\n",
    "        rank = 1/rank\n",
    "        #print(rank)\n",
    "        total = np.sum(rank**0.6)\n",
    "        numerator = rank**0.6\n",
    "        dist = numerator/total\n",
    "        \n",
    "        \n",
    "        cum_dist = [0]*len(dist)\n",
    "        cum_dist[0] = dist[0]\n",
    "        for i in range(1,len(dist)):\n",
    "            cum_dist[i] = cum_dist[i-1] + dist[i]\n",
    "        #print(cum_dist)\n",
    "        unif = np.random.uniform(0,1,size = int(np.ceil(len(memory1)**(2/np.pi))))\n",
    "        choices = []\n",
    "        for u in range(len(unif)):\n",
    "            choi = 0 \n",
    "            for j in range(len(cum_dist)):\n",
    "                if unif[u] > cum_dist[j]:\n",
    "                    choi += 1\n",
    "            choices.append(choi)\n",
    "        #print(choices)    \n",
    "        temp_mem = np.array(memory1)[choices]\n",
    "        #for q in choices:\n",
    "        #    temp_mem.append(memory1[q])\n",
    "        #plt.bar(x = range(0,len(dist)), height = dist)\n",
    "        #plt.show()\n",
    "        #temp_mem = random.sample(memory1,20)\n",
    "        \n",
    "        \n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        \n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model2.predict(new_state)[0][0]))\n",
    "            if done == True:\n",
    "                update = reward\n",
    "            q_vals = model1.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            #q_val_list.append(q_vals)\n",
    "            #states.append(state)\n",
    "            model1.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (len(temp_mem),1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (len(temp_mem),1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model1.fit(states,q_val_list,verbose = 0)\n",
    "        if done == True:\n",
    "            s += 1\n",
    "            game_count1 += 1\n",
    "            '''\n",
    "            model_json = model1.to_json()\n",
    "            with open(\"model1.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model1.save_weights(\"model1.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            '''\n",
    "            print('Game Count: ' + str(game_count1) + ', length of game: ' + str(_-save))\n",
    "            save = _\n",
    "            if game_count1 == 10000:\n",
    "                break\n",
    "            state = env.reset()\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#game_state_save = game_count1\n",
    "#game_count1 = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "save = 0\n",
    "lens = []\n",
    "for _ in range(300):\n",
    "    state = np.reshape(state, [1,1, observation_space])\n",
    "    action = np.argmax(model1.predict(state)[0])\n",
    "    #print(model1.predict(state))\n",
    "    #print(action)\n",
    "    \n",
    "    env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        #print(_-save)\n",
    "        lens.append(_-save)\n",
    "        save = _\n",
    "        \n",
    "        #env.close()\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    #memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "env.close()\n",
    "plt.hist(lens)\n",
    "plt.show()\n",
    "print(np.max(lens),np.std(lens),np.median(lens),np.mean(lens))\n",
    "#game_count1 = game_state_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_count = 0\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]\n",
    "#print(observation_space)\n",
    "\n",
    "state = env.reset()\n",
    "memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "s = 0\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model.predict(state)[0][0])\n",
    "    if np.random.uniform(0,1) > 0.9:\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count += 1\n",
    "        print(game_count)\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1,observation_space])\n",
    "    memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory) >= 20:\n",
    "        temp_mem = random.sample(memory,20)\n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model.predict(new_state)[0][0]))\n",
    "            q_vals = model.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            states.append(state)\n",
    "            model.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous train!\n",
    "s = 0\n",
    "#memory = deque(maxlen = 1000000)\n",
    "for _ in range(100000):\n",
    "    state = np.reshape(state, [1,1,observation_space])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    if np.random.uniform(0,1) > (game_count+1)/(game_count+100):\n",
    "        action = env.action_space.sample()\n",
    "    #env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        s += 1\n",
    "        game_count += 1\n",
    "        print(game_count)\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "    if len(memory) >= 20:\n",
    "        temp_mem = random.sample(memory,20)\n",
    "        q_val_list = []\n",
    "        states = []\n",
    "        for new_state, state, reward, action in temp_mem:\n",
    "            update = reward + 0.95*(np.amax(model.predict(new_state)[0][0]))\n",
    "            q_vals = model.predict(state)[0][0]\n",
    "            q_vals[action] = update\n",
    "            q_vals = np.reshape(q_vals,(1,1,2))\n",
    "            q_val_list.append(q_vals)\n",
    "            states.append(state)\n",
    "            model.fit(state,q_vals,verbose = 0)\n",
    "            #print(q_vals)\n",
    "        #print(np.array(states))\n",
    "        #states = np.reshape(np.array(states),[20,1,4])\n",
    "        #states = np.array([item for sublist in states for item in sublist])\n",
    "        #states = np.reshape(states, (20,1,4))\n",
    "        #q_val_list = np.reshape(q_val_list, (20,1,2))\n",
    "        #print(states)\n",
    "        #print(q_val_list)\n",
    "        #print('_____________')\n",
    "        #states = temp_mem[]\n",
    "        #model.fit(states,q_val_list,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#memory = deque(maxlen = 1000000)\n",
    "#obs = env.step(env.action_space.sample())\n",
    "\n",
    "for _ in range(1000):\n",
    "    state = np.reshape(state, [1,1, observation_space])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    print(model.predict(state))\n",
    "    print(action)\n",
    "    env.render()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        #print(_)\n",
    "        #env.close()\n",
    "        state = env.reset()\n",
    "        continue\n",
    "    new_state = np.reshape(new_state, [1,1, observation_space])\n",
    "    #memory.append((new_state,state,reward,action))\n",
    "    state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
